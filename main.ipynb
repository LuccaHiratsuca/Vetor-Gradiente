{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APS 2 - Otimização pelo Vetor Gradiente\n",
    "\n",
    "## Grupo: 5\n",
    "## Participantes:\n",
    "\n",
    "- Alexandre Wever\n",
    "- Gabriel Mendes\n",
    "- Lucca Hiratsuca Costa\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importando bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarefa 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Considere a função f cuja lei f(x, y) será atribuída a seu grupo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a) Construa o gráfico de f no GeoGebra e observe que f possui um único ponto de mínimo e não possui pontos de máximo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <img src=\"./assets/tarefa1_questaoA.jpg\" alt=\"tarefa1_questaoA\" width=\"900\" height=\"400\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b) Determine o vetor gradiente de fem um ponto genérico (x, y)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <img src=\"./assets/tarefa1_questaoB.jpg\" alt=\"tarefa1_questaoB\" width=\"800\" height=\"400\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### c) Usando as ideias desenvolvidas na página anterior, elabore um código que permita determinar o ponto de mínimo da função f. Utilize um passo fixo a = 0,1 e a estimativa inicial (xo, Yo) = (0,0). A precisão do cálculo deverá ser de 10-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum point: (-0.46250571330191653, -0.06734189644276807, -0.26665033899236973), reached in 23 steps.\n"
     ]
    }
   ],
   "source": [
    "def f(x, y):\n",
    "    \"\"\"Define a função f(x, y).\"\"\"\n",
    "    return x**2 + x*y + 4*y**2 + x + y\n",
    "\n",
    "def grad_f(x, y):\n",
    "    \"\"\"Calcula o gradiente da função no ponto (x, y).\"\"\"\n",
    "    df_dx = 2*x + y + 1\n",
    "    df_dy = x + 8*y + 1\n",
    "    return np.array([df_dx, df_dy])\n",
    "\n",
    "def gradient_descent(function, grad, initial_point, step_size, precision):\n",
    "    \"\"\"Executa o método do gradiente descendente para encontrar o ponto de mínimo da função especificada.\"\"\"\n",
    "    current_point = np.array(initial_point)\n",
    "    difference = np.inf\n",
    "    steps = 0\n",
    "\n",
    "    while difference > precision:\n",
    "        previous_point = current_point\n",
    "        current_point = current_point - step_size * grad(*current_point)\n",
    "        difference = np.abs(function(*current_point) - function(*previous_point))\n",
    "        steps += 1\n",
    "\n",
    "    return current_point, steps\n",
    "\n",
    "# Parâmetros do algoritmo\n",
    "initial_point = (0, 0)\n",
    "step_size = 0.1\n",
    "precision = 1e-5\n",
    "\n",
    "# Executa o algoritmo\n",
    "min_point, steps = gradient_descent(f, grad_f, initial_point, step_size, precision)\n",
    "z_min = f(*min_point)\n",
    "min = (*min_point, z_min)\n",
    "\n",
    "print(f\"Minimum point: {min}, reached in {steps} steps.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### d) Repita o procedimento para os seguintes valores do passo: a = 0,15, a = 0,2, a = 0,3 e a = 0,5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha: 0.15, Minimum point: (-0.4641161580836166, -0.06708055722590127, -0.26666053198247813), Number of steps: 16\n",
      "Alpha: 0.2, Minimum point: (-0.46475520000000003, -0.06639360000000002, -0.2666621927424), Number of steps: 12\n",
      "Alpha: 0.3, Minimum point: (1.6178209494833905e+153, 9.969461895153895e+153, inf), Number of steps: 962\n",
      "Alpha: 0.5, Minimum point: (5.789639271493776e+153, 3.5677364743159627e+154, inf), Number of steps: 318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0b/t7v75s011nxd0q97zmc5_fm80000gn/T/ipykernel_77858/724991345.py:3: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  return x**2 + x*y + 4*y**2 + x + y\n",
      "/var/folders/0b/t7v75s011nxd0q97zmc5_fm80000gn/T/ipykernel_77858/724991345.py:20: RuntimeWarning: invalid value encountered in scalar subtract\n",
      "  difference = np.abs(function(*current_point) - function(*previous_point))\n",
      "/var/folders/0b/t7v75s011nxd0q97zmc5_fm80000gn/T/ipykernel_77858/724991345.py:3: RuntimeWarning: overflow encountered in scalar power\n",
      "  return x**2 + x*y + 4*y**2 + x + y\n"
     ]
    }
   ],
   "source": [
    "alphas = [0.15, 0.2, 0.3, 0.5]\n",
    "initial_point = (0, 0)  # Ponto inicial\n",
    "precision = 1e-5  # Precisão\n",
    "\n",
    "# Testando diferentes valores de alpha\n",
    "for alpha in alphas:\n",
    "    min_point, steps = gradient_descent(f, grad_f, initial_point, alpha, precision)\n",
    "    z_min = f(*min_point)\n",
    "    min = (*min_point, z_min)\n",
    "\n",
    "    print(f\"Alpha: {alpha}, Minimum point: {min}, Number of steps: {steps}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assim como podemos observar, para os valores de $\\alpha$: 0,3 e 0,5, o resultado acabou resultando em `overflow`.\n",
    "\n",
    "**O que isso significa?**\n",
    "\n",
    "Quando o tamanho do passo é grande (como 0.3 ou 0.5 neste caso), a atualização do ponto atual no método do gradiente descendente pode levar a um ponto muito distante do ponto anterior, o que acaba causando instabilidade no algoritmo.\n",
    "\n",
    "Devido a esses grandes passos, o ponto atual pode \"pular\" sobre o mínimo e se mover cada vez mais longe dele. Em vez de convergir para o mínimo, os valores de $x$ e $y$ crescem exponencialmente.\n",
    "\n",
    "Ou seja, à medida que $x$ e $y$ crescem exponencialmente, a função: $f(x, y)$ e suas derivadas também crescem exponencialmente, eventualmente atingindo valores que são demasiado grandes para serem representados na memória do computador. \n",
    "\n",
    "E, isso resulta em um \"overflow\", onde os valores são tão grandes que são considerados infinitos ou indeterminados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarefa 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Utilize o seu código para determinar os pontos de mínimo da função g, cuja lei g(x, y) será atribuída a seu grupo. \n",
    "\n",
    "A função g possui dois pontos de mínimo. No relatório, deverá aparecer:\n",
    "\n",
    "- os dois pontos de mínimo obtidos com seu código;\n",
    "- o que foi modificado no programa para obter o segundo ponto de mínimo;\n",
    "- o comportamento do processo de convergência para diferentes valores do passo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <img src=\"./assets/tarefa2.jpg\" alt=\"tarefa2\" width=\"800\" height=\"300\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acima, foi plotado no Geogebra a função $g(x, y)$ para termos uma ideia e para termos uma noção de como ela se comporta e uma estimativa dos valores dos pontos de mínimo que devem ser encontrados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum point: [ 2.58367765 -1.96659132], Function value: 3.8545048561043367, Reached in 117 steps.\n",
      "Minimum point: [2.58372946 1.96665774], Function value: 3.854505532892432, Reached in 118 steps.\n"
     ]
    }
   ],
   "source": [
    "def g(x1, x2):\n",
    "    \"\"\"Define a função g(x1, x2).\"\"\"\n",
    "    return np.sqrt(x1**2 + x2**2 + 2) + x1**2 * np.exp(-x2**2) + (x1 - 3)**2\n",
    "\n",
    "def grad_g(x1, x2):\n",
    "    \"\"\"Calcula o gradiente da função no ponto (x1, x2).\"\"\"\n",
    "    dg_dx1 = (x1 / np.sqrt(x1**2 + x2**2 + 2)) + 2*x1*np.exp(-x2**2) + 2*(x1 - 3)\n",
    "    dg_dx2 = (x2 / np.sqrt(x1**2 + x2**2 + 2)) - 2*x1**2*x2*np.exp(-x2**2)\n",
    "    return np.array([dg_dx1, dg_dx2])\n",
    "\n",
    "def gradient_descent(function, grad, initial_points, step_size, precision):\n",
    "    \"\"\"Executa o método do gradiente descendente para encontrar os pontos de mínimo da função especificada.\"\"\"\n",
    "    minima = []\n",
    "\n",
    "    for initial_point in initial_points:\n",
    "        current_point = np.array(initial_point)\n",
    "        difference = np.inf\n",
    "        steps = 0\n",
    "\n",
    "        while difference > precision:\n",
    "            previous_point = current_point\n",
    "            current_point = current_point - step_size * grad(*current_point)\n",
    "            difference = np.abs(function(*current_point) - function(*previous_point))\n",
    "            steps += 1\n",
    "\n",
    "        minima.append((current_point, steps))\n",
    "\n",
    "    return minima\n",
    "\n",
    "# Parâmetros do algoritmo\n",
    "initial_points = [(-10, -10), (10, 10)]\n",
    "step_size = 0.1\n",
    "precision = 1e-5\n",
    "\n",
    "# Executa o algoritmo\n",
    "minima = gradient_descent(g, grad_g, initial_points, step_size, precision)\n",
    "minima_info = [(point, g(*point), steps) for point, steps in minima]\n",
    "\n",
    "for point, value, steps in minima_info:\n",
    "    print(f\"Minimum point: {point}, Function value: {value}, Reached in {steps} steps.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**O que foi modificado no programa para obter o segundo ponto de mínimo?**\n",
    "\n",
    "- Modificação no Método Gradient Descent: O método 'gradient_descent foi adaptado para aceitar múltiplos pontos iniciais. Isso permite que o algoritmo explore diferentes regiões do espaço de entrada para encontrar múltiplos pontos de mínimo.\n",
    "- Alteração nos Parâmetros Iniciais: Foram utilizados diferentes pontos iniciais, `initial_points = [(-10, -10), (10, 10)] `. Isso permite que o algoritmo de descida do gradiente comece a busca em diferentes locais no espaço da função.\n",
    "- Armazenamento de Múltiplos Mínimos: O algoritmo foi alterado para retornar uma lista de mínimos encontrados a partir dos diferentes pontos iniciais. Cada mínimo é acompanhado pelo número de passos necessários para alcançá-lo.\n",
    "\n",
    "Essas modificações permitem que o algoritmo explore de forma mais abrangente o espaço da função g(x1, x2) e encontre múltiplos pontos de mínimo, contrastando com a abordagem anterior que focava em encontrar um único mínimo para a função f(x, y)'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### O comportamento do processo de convergência para diferentes valores do passo:\n",
    "\n",
    "Os resultados em relação ao passo são de que: \n",
    "- Quanto menor o passo, maior é o número de vezes que o $while$ roda e com isso, tendo um resultado mais preciso. Ou seja, há um maior número de iterações.\n",
    "- Por outro lado, com um passo maior, o número de iterações é menor, porém, o resultado que alcançamos é menos preciso e, dependendo do valor do passo, pode até resultar em um overflow (como visto anteriormente), devido ao tamanho do passo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarefa 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Utilize o seu código para determinar os dois pontos de máximo da função h, cuja lei h(x,y) será atribuída a seu grupo. No relatório, indique os dois pontos de máximo obtidos com seu código."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "    <img src=\"./assets/tarefa3.jpg\" alt=\"tarefa3\" width=\"1000\" height=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Da mesma forma que na tarefa 2, foi plotado no Geogebra a função $h(x, y)$ para termos uma ideia e para termos uma noção de como ela se comporta e uma estimativa dos valores dos pontos de máximo que devem ser encontrados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum point: [-0.00996705 -0.03751632], Function value: 5.993869515841805, Reached in 123697 steps.\n",
      "Maximum point: [1.93361297 2.97956814], Function value: 3.8840861904994943, Reached in 61943 steps.\n"
     ]
    }
   ],
   "source": [
    "def h(x, y):\n",
    "    \"\"\"Define a função h(x, y).\"\"\"\n",
    "    return 4*np.exp(-x**2 - y**2) + 3*np.exp(-x**2 - y**2 + 4*x + 6*y - 13) - x**2/8 - y**2/14 + 2\n",
    "\n",
    "def grad_h(x, y):\n",
    "    \"\"\"Calcula o gradiente da função no ponto (x, y).\"\"\"\n",
    "    dh_dx = -8*x*np.exp(-x**2 - y**2) - 6*x*np.exp(-x**2 - y**2 + 4*x + 6*y - 13) - x/4 + 12*np.exp(-x**2 - y**2 + 4*x + 6*y - 13)\n",
    "    dh_dy = -8*y*np.exp(-x**2 - y**2) - 6*y*np.exp(-x**2 - y**2 + 4*x + 6*y - 13) - y/7 + 18*np.exp(-x**2 - y**2 + 4*x + 6*y - 13)\n",
    "    return np.array([dh_dx, dh_dy])\n",
    "\n",
    "def gradient_ascent(function, grad, initial_points, step_size, precision):\n",
    "    \"\"\"Executa o método do gradiente ascendente para encontrar os pontos de máximo da função especificada.\"\"\"\n",
    "    maxima = []\n",
    "\n",
    "    for initial_point in initial_points:\n",
    "        current_point = np.array(initial_point)\n",
    "        difference = np.inf\n",
    "        steps = 0\n",
    "\n",
    "        while difference > precision:\n",
    "            previous_point = current_point\n",
    "            current_point = current_point + step_size * grad(*current_point)\n",
    "            difference = np.abs(function(*current_point) - function(*previous_point))\n",
    "            steps += 1\n",
    "\n",
    "        maxima.append((current_point, steps))\n",
    "\n",
    "    return maxima\n",
    "\n",
    "# Parâmetros do algoritmo\n",
    "initial_points = [(-10, -10), (10, 10)]  # Escolhemos dois pontos iniciais arbitrários\n",
    "step_size = 0.0001\n",
    "precision = 1e-5\n",
    "\n",
    "# Executa o algoritmo\n",
    "maxima = gradient_ascent(h, grad_h, initial_points, step_size, precision)\n",
    "maxima_info = [(point, h(*point), steps) for point, steps in maxima]\n",
    "\n",
    "for point, value, steps in maxima_info:\n",
    "    print(f\"Maximum point: {point}, Function value: {value}, Reached in {steps} steps.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observarmos os valores encontrados pela função e compararmos com a imagem, podemos ver que os valores encontrados são quase os mesmos! Em outras palavras, nossa estimativa foi bem próxima do valor real."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarefa Bônus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Faça uma pesquisa sobre a utilização de passo variável e implemente o método no seu código para calcular o ponto de mínimo da função f da Tarefa 1. Compare o número de iterações necessárias até a convergência nos dois casos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiramente, vamos entender o que é o passo variável:\n",
    "\n",
    "O passo variável, no contexto de algoritmos de otimização como o gradiente descendente, refere-se à adaptação dinâmica do tamanho do passo (também conhecido como taxa de aprendizado) em cada iteração do algoritmo. Diferentemente de um passo fixo, onde o tamanho do passo permanece constante ao longo de todas as iterações, um passo variável ajusta-se de acordo com certas condições ou critérios para melhorar a eficiência do algoritmo.\n",
    "\n",
    "Esse possui algumas características como:\n",
    "\n",
    "- **Ajuste Dinâmico:** O tamanho do passo é modificado a cada iteração com base na resposta da função objetivo ou nas propriedades do gradiente.\n",
    "\n",
    "- **Eficiência de Convergência:** Algoritmos com passo variável frequentemente convergem mais rápido que aqueles com passo fixo, especialmente em funções complexas ou com gradientes que mudam rapidamente.\n",
    "- **Métodos de Ajuste:** Existem várias estratégias para ajustar o tamanho do passo, como:\n",
    "    - Backtracking Line Search: Reduz o passo até que uma redução suficiente na função seja alcançada.\n",
    "    \n",
    "    - Condição de Armijo: Uma condição específica que avalia se o decréscimo na função é proporcional ao tamanho do passo e ao quadrado da norma do gradiente.\n",
    "    - Como o AdaGrad, RMSprop, e Adam, comuns em aprendizado de máquina, que ajustam o passo com base em propriedades estatísticas dos gradientes.\n",
    "\n",
    "- **Estabilidade e precisão:** Ajustar o passo pode ajudar a evitar passos muito grandes, que podem levar a instabilidades ou oscilações, e passos muito pequenos, que podem causar lentidão na convergência.\n",
    "\n",
    "- **Dependência do problema:** A eficácia de um passo variável pode depender fortemente da natureza específica da função objetivo e do espaço de soluções.\n",
    "\n",
    "\n",
    "Em suma, a utilização de um passo variável é uma técnica avançada em otimização que pode significativamente melhorar a performance de algoritmos de gradiente descendente, especialmente em cenários onde a topologia da função objetivo é complexa ou quando se deseja uma convergência mais rápida e estável.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum point: (-0.46354845414328, -0.0671726828988696, -0.26665749707376896), reached in 19 steps.\n"
     ]
    }
   ],
   "source": [
    "def f(x, y):\n",
    "    \"\"\"Define a função f(x, y).\"\"\"\n",
    "    return x**2 + x*y + 4*y**2 + x + y\n",
    "\n",
    "def grad_f(x, y):\n",
    "    \"\"\"Calcula o gradiente da função no ponto (x, y).\"\"\"\n",
    "    df_dx = 2*x + y + 1\n",
    "    df_dy = x + 8*y + 1\n",
    "    return np.array([df_dx, df_dy])\n",
    "\n",
    "def gradient_descent(function, grad, initial_point, initial_step_size, precision, armijo_constant=0.5, reduction_factor=0.5):\n",
    "    \"\"\"Executa o método do gradiente descendente com passo variável para encontrar o ponto de mínimo da função especificada.\"\"\"\n",
    "    current_point = np.array(initial_point)\n",
    "    step_size = initial_step_size\n",
    "    difference = np.inf\n",
    "    steps = 0\n",
    "\n",
    "    while difference > precision:\n",
    "        previous_point = current_point\n",
    "        gradient = grad(*current_point)\n",
    "        current_point = current_point - step_size * gradient\n",
    "        while function(*current_point) > function(*previous_point) - armijo_constant * step_size * np.dot(gradient, gradient):\n",
    "            step_size *= reduction_factor\n",
    "            current_point = previous_point - step_size * gradient\n",
    "        difference = np.abs(function(*current_point) - function(*previous_point))\n",
    "        steps += 1\n",
    "\n",
    "    return current_point, steps\n",
    "\n",
    "# Parâmetros do algoritmo\n",
    "initial_point = (0, 0)\n",
    "initial_step_size = 1.0\n",
    "precision = 1e-5\n",
    "\n",
    "# Executa o algoritmo\n",
    "min_point, steps = gradient_descent(f, grad_f, initial_point, initial_step_size, precision)\n",
    "z_min = f(*min_point)\n",
    "min = (*min_point, z_min)\n",
    "\n",
    "print(f\"Minimum point: {min}, reached in {steps} steps.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste código, foi implementado o passo variável, ajustando o tamanho do passo (valendo inicialmente 1) e agora, a cada iteração, nós ajustamos esse valor baseado em algum critério. Nesse caso, com base na regra de Armijo, o qual ajusta o tamanho do passo para garantir que a função objetivo seja reduzida em uma quantidade suficiente.\n",
    "\n",
    "Sabendo disso, o `step_size` é inicialmente definido para um valor maior (1.0), e a cada iteração, ele é reduzido multiplicativamente por `reduction_factor` até que a condição de Armijo seja satisfeita. \n",
    "\n",
    "Obs.: A constante de Armijo `armijo_constant` determina quão agressiva é a redução na função objetivo necessária para aceitar o novo ponto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compare o número de iterações necessárias até a convergência nos dois casos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Definição das funções f e grad_f\n",
    "def f(x, y):\n",
    "    return x**2 + x*y + 4*y**2 + x + y\n",
    "\n",
    "def grad_f(x, y):\n",
    "    df_dx = 2*x + y + 1\n",
    "    df_dy = x + 8*y + 1\n",
    "    return np.array([df_dx, df_dy])\n",
    "\n",
    "# Método de gradiente descendente com passo fixo\n",
    "def gradient_descent_fixed_step(function, grad, initial_point, step_size, precision):\n",
    "    current_point = np.array(initial_point)\n",
    "    difference = np.inf\n",
    "    steps = 0\n",
    "\n",
    "    while difference > precision:\n",
    "        previous_point = current_point\n",
    "        current_point = current_point - step_size * grad(*current_point)\n",
    "        difference = np.abs(function(*current_point) - function(*previous_point))\n",
    "        steps += 1\n",
    "\n",
    "    return current_point, steps\n",
    "\n",
    "# Método de gradiente descendente com passo variável\n",
    "def gradient_descent_variable_step(function, grad, initial_point, initial_step_size, precision, armijo_constant=0.5, reduction_factor=0.5):\n",
    "    current_point = np.array(initial_point)\n",
    "    step_size = initial_step_size\n",
    "    difference = np.inf\n",
    "    steps = 0\n",
    "\n",
    "    while difference > precision:\n",
    "        previous_point = current_point\n",
    "        gradient = grad(*current_point)\n",
    "        current_point = current_point - step_size * gradient\n",
    "        while function(*current_point) > function(*previous_point) - armijo_constant * step_size * np.dot(gradient, gradient):\n",
    "            step_size *= reduction_factor\n",
    "            current_point = previous_point - step_size * gradient\n",
    "        difference = np.abs(function(*current_point) - function(*previous_point))\n",
    "        steps += 1\n",
    "\n",
    "    return current_point, steps\n",
    "\n",
    "# Parâmetros do algoritmo\n",
    "initial_point = (0, 0)\n",
    "fixed_step_size = 0.1\n",
    "variable_initial_step_size = 1.0\n",
    "precision = 1e-5\n",
    "\n",
    "# Execução dos métodos\n",
    "fixed_min_point, fixed_steps = gradient_descent_fixed_step(f, grad_f, initial_point, fixed_step_size, precision)\n",
    "variable_min_point, variable_steps = gradient_descent_variable_step(f, grad_f, initial_point, variable_initial_step_size, precision)\n",
    "\n",
    "# Plotando a comparação\n",
    "plt.bar(['Fixed Step', 'Variable Step'], [fixed_steps, variable_steps])\n",
    "plt.ylabel('Number of Iterations')\n",
    "plt.title('Comparison of Gradient Descent Iterations')\n",
    "for i, v in enumerate([fixed_steps, variable_steps]):\n",
    "    plt.text(i - 0.1, v + 0.1, str(v))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assim como dito anteriormente, quando temos um passo variável, o número de iterações é menor, pois o passo é ajustado de acordo com a resposta da função objetivo ou nas propriedades do gradiente. \n",
    "\n",
    "Com essa comparação mostrada no gráfico, podemos ver essa diferença de eficiência entre o passo fixo e o passo variável, dado que estamos utilizando a mesma função e os mesmos parâmetros e a única diferença é o tipo de passo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
